ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for Various Cutoffs")
ggsave("images/cv_p_2.png", width = 8, height = 5)
model_glm_two <- glm(as.numeric(PD == "Yes")~age+ejection_fraction+serum_creatinine, family = binomial(logit), data = heart_train)
preds_glm_two <- predict(model_glm_two, heart_test)
preds_glm_two <- ifelse(preds_glm_two > opt_p_two, "Yes", "No") %>% factor(levels = c("Yes","No"))
cm_glm_two <- confusionMatrix(preds_glm_two, heart_test$PD)
accuracy_results <- data.frame(Method = "GLM",
accuracy = cm_glm_two$overall["Accuracy"],
sensitivity = cm_glm_two$byClass["Sensitivity"])
View(accuracy_results)
set.seed(1, sample.kind = "Rounding")
model_nb <- train(PD~.,
method = 'nb',
data = heart_train,
trControl = trainControl(method = 'cv', number = 10))
#Plot results
ggplot(model_nb)
ggsave("images/cv_nb.png", width = 8, height = 5)
#create predictions
preds_nb <- predict(model_nb, heart_test)
#confusion matrix
cm_nb <- confusionMatrix(preds_nb, heart_test$PD)
#store results
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Naive Bayes",
accuracy = cm_nb$overall["Accuracy"],
sensitivity = cm_nb$byClass["Sensitivity"]))
k <- 10
cp = seq(0,0.5,0.01)
accuracy_cp <- matrix(nrow = k, ncol = length(cp))
#create folds
set.seed(2005, sample.kind = "Rounding")
ind <- createFolds(1:nrow(heart_train), k = k)
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_cp[i,] <- sapply(cp, function(cp){
# create the decision tree
cv_mod_tree <- rpart(PD~., cp = cp, data =  cv_train, method = "class")
# obtain the predictions (these are probabilities)
cv_preds_tree <- predict(cv_mod_tree, cv_test, type = "class")
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_tree, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
#extract optimal cp
opt_cp <- median(cp[which(min_rank(desc(colMeans(accuracy_cp)))==1)])
#plot cp vs accuracy
tibble(cp = cp, mean_accuracy = colMeans(accuracy_cp)) %>%
ggplot(aes(cp, mean_accuracy)) +
geom_smooth()+
geom_point() +
geom_point(aes(opt_cp, max(mean_accuracy)), shape = 5, size = 5) +
xlab("Cutoff (cp)") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for Various complex. params")
ggsave("images/cv_dt.png", width = 8, height = 5)
#create model using optimal tree
model_tree <- rpart(PD~., cp = opt_cp, data =  heart_train, method = "class")
#plot the model - this really helps to understand how the algorithm works
rpart.plot(model_tree, type = 0)
#generate predictions
preds_tree <- predict(model_tree, heart_test, type = "class")
#confusion matrix
cm_tree <- confusionMatrix(preds_tree, heart_test$PD)
# save accuracy and sensitivity
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Decision Tree",
accuracy = cm_tree$overall["Accuracy"],
sensitivity = cm_tree$byClass["Sensitivity"]))
k <- 10
mtry = 1:11
accuracy_mtry <- matrix(nrow = k, ncol = length(mtry))
set.seed(262, sample.kind = "Rounding")
ind <- createFolds(1:nrow(heart_train), k = k)
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_mtry[i,] <- sapply(mtry, function(m){
# create the decision tree
cv_mod_rf <- randomForest(PD ~.,
data = cv_train)
# obtain the predictions (these are probabilities)
cv_preds_tree <- predict(cv_mod_rf, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_rf, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_mtry[i,] <- sapply(mtry, function(m){
# create the decision tree
cv_mod_rf <- randomForest(PD ~.,
data = cv_train)
# obtain the predictions (these are probabilities)
cv_preds_rf <- predict(cv_mod_rf, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_rf, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_mtry[i,] <- sapply(mtry, function(m){
# create the decision tree
cv_mod_rf <- randomForest(PD ~.,
data = cv_train)
# obtain the predictions (these are probabilities)
cv_preds_rf <- predict(cv_mod_rf, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_rf, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
opt_mtry <- median(cp[which(min_rank(desc(colMeans(accuracy_mtry)))==1)])
opt_mtry <- median(mtry[which(min_rank(desc(colMeans(accuracy_mtry)))==1)])
tibble(mtry = mtry, mean_accuracy = colMeans(accuracy_mtry)) %>%
ggplot(aes(mtry, mean_accuracy)) +
geom_smooth()+
geom_point() +
geom_point(aes(opt_mtry, max(mean_accuracy)), shape = 5, size = 5) +
xlab("Cutoff (cp)") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
ggsave("images/cv_tree.png", width = 8, height = 5)
tibble(mtry = mtry, mean_accuracy = colMeans(accuracy_mtry)) %>%
ggplot(aes(mtry, mean_accuracy)) +
geom_line()+
geom_point() +
geom_point(aes(opt_mtry, max(mean_accuracy)), shape = 5, size = 5) +
xlab("Cutoff (cp)") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
ggsave("images/cv_tree.png", width = 8, height = 5)
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_mtry[i,] <- sapply(mtry, function(m){
# create the decision tree
set.seed(262, sample.kind = "Rounding")
cv_mod_rf <- randomForest(PD ~.,
data = cv_train,
mtry = m)
# obtain the predictions (these are probabilities)
cv_preds_rf <- predict(cv_mod_rf, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_rf, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
opt_mtry <- median(mtry[which(min_rank(desc(colMeans(accuracy_mtry)))==1)])
tibble(mtry = mtry, mean_accuracy = colMeans(accuracy_mtry)) %>%
ggplot(aes(mtry, mean_accuracy)) +
geom_line()+
geom_point() +
geom_point(aes(opt_mtry, max(mean_accuracy)), shape = 5, size = 5) +
xlab("Cutoff (cp)") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
ggsave("images/cv_tree.png", width = 8, height = 5)
model_rf <- randomForest(PD ~.,
data = heart_train,
mtry = opt_mtry)
preds_rf <- predict(model_rf, heart_test)
cm_rf <- confusionMatrix(preds_rf,heart_test$PD)
cm_rf <- confusionMatrix(preds_rf,heart_test$PD)
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Random Forest",
accuracy = cm_rf$overall["Accuracy"],
sensitivity = cm_rf$byClass["Sensitivity"]))
View(accuracy_results)
tibble(mtry = mtry, mean_accuracy = colMeans(accuracy_mtry)) %>%
ggplot(aes(mtry, mean_accuracy)) +
geom_line()+
geom_point() +
geom_point(aes(opt_mtry, max(mean_accuracy)), shape = 5, size = 5) +
xlab("mtry") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
ggsave("images/cv_tree.png", width = 8, height = 5)
tibble(mtry = mtry, mean_accuracy = colMeans(accuracy_mtry)) %>%
ggplot(aes(mtry, mean_accuracy)) +
geom_line()+
geom_point() +
geom_point(aes(opt_mtry, max(mean_accuracy)), shape = 5, size = 5) +
xlab("mtry") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
ggsave("images/cv_rf.png", width = 8, height = 5)
seq(0.1,10,0.1)
k <- 10
cost = seq(0.01, 1, 0.01)
accuracy_cost <- matrix(nrow = k, ncol = length(cost))
set.seed(2014, sample.kind = "Rounding")
ind <- createFolds(1:nrow(heart_train), k = k)
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_cost[i,] <- sapply(cost, function(c){
# create the decision tree
cv_mod_svm <- svm(PD ~.,
data = cv_train,
scale = T,
center = T,
kernel = "linear",
cost = c)
# obtain the predictions (these are probabilities)
cv_preds_svm <- predict(cv_mod_svm, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_svm, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
opt_cost <- median(cost[which(min_rank(desc(colMeans(accuracy_cost)))==1)])
#plot mtry vs accuracy
tibble(cost = cost, mean_accuracy = colMeans(accuracy_cost)) %>%
ggplot(aes(cost, mean_accuracy)) +
geom_smooth()+
geom_point() +
geom_point(aes(opt_cost, max(mean_accuracy)), shape = 5, size = 5) +
xlab("cost") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
model_svm <- svm(PD ~.,
data = train,
scale = T,
center = T,
kernel = "linear",
cost = opt_cost)
preds_svm <- predict(model_svm, heart_test)
cm_svm <- confusionMatrix(preds_svm, heart_test$PD)
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Support Vector Machine",
accuracy = cm_svm$overall["Accuracy"],
sensitivity = cm_svm$byClass["Sensitivity"]))
accuracy_results %>% mutate(mean_acc_sens = (accuracy + sensitivity)/2) %>% arrange(mean_acc_sens) # top 3 is Naive Bayes, Decision Tree and Random Forest
for (i in 1:k) {
# create train and test sets for cv
cv_train <- heart_train %>% slice(-ind[[i]])
cv_test <- heart_train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_cost[i,] <- sapply(cost, function(c){
# create the decision tree
cv_mod_svm <- svm(PD ~.,
data = cv_train,
scale = T,
center = T,
kernel = "linear",
cost = c)
# obtain the predictions (these are probabilities)
cv_preds_svm <- predict(cv_mod_svm, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_svm, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
opt_cost <- median(cost[which(min_rank(desc(colMeans(accuracy_cost)))==1)])
#plot mtry vs accuracy
tibble(cost = cost, mean_accuracy = colMeans(accuracy_cost)) %>%
ggplot(aes(cost, mean_accuracy)) +
geom_smooth()+
geom_point() +
geom_point(aes(opt_cost, max(mean_accuracy)), shape = 5, size = 5) +
xlab("cost") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various mtry")
ggsave("images/cv_svm.png", width = 8, height = 5)
tibble(cost = cost, mean_accuracy = colMeans(accuracy_cost)) %>%
ggplot(aes(cost, mean_accuracy)) +
geom_line()+
geom_point() +
geom_point(aes(opt_cost, max(mean_accuracy)), shape = 5, size = 5) +
xlab("cost") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various cost values")
ggsave("images/cv_svm.png", width = 8, height = 5)
plot mtry vs accuracy
tibble(cost = cost, mean_accuracy = colMeans(accuracy_cost)) %>%
ggplot(aes(cost, mean_accuracy)) +
geom_smooth()+
geom_point() +
geom_point(aes(opt_cost, max(mean_accuracy)), shape = 5, size = 5) +
xlab("cost") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for various cost values")
ggsave("images/cv_svm.png", width = 8, height = 5)
all_preds <- tibble(nb = preds_nb,
tree = preds_tree,
rf  = preds_rf)
preds_ens <- apply(all_preds,1,function(x) names(which.max(table(x)))) %>%
factor(levels = c("Yes","No"))
cm_ens <- confusionMatrix(preds_ens, heart_test$PD)
# save accuracy and sensitivity
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Ensemble",
accuracy = cm_ens$overall["Accuracy"],
sensitivity = cm_ens$byClass["Sensitivity"]))
results <- tibble(Method = c("Logistic Regression", "Naive Bayes", "Decision Tree","Random Forest","SVM","Ensemble"),
Accuracy = c(confusionMatrix(preds_glm, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_nb, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_tree, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_rf, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_svm, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_ens, heart_test$PD)$overall["Accuracy"]),
Sensitivity = c(confusionMatrix(preds_glm, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_nb, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_tree, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_rf, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_svm, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_ens, heart_test$PD)$byClass["Sensitivity"])) %>%
mutate(Mean = rowMeans(dplyr::select(., Accuracy, Sensitivity)))
accuracy_results %>%
dplyr::select(accuracy, sensitivity) %>%
mutate(means = rowMeans(.))
accuracy_results %>% mutate(mean_acc_sens = (accuracy + sensitivity)/2) %>% arrange(mean_acc_sens) # top 3 is Naive Bayes, Decision Tree and Random Forest
rpart.plot(model_tree, type = 0)
tibble(nb = preds_nb,
tree = preds_tree,
rf  = preds_rf)
preds_ens <- apply(all_preds,1,function(x) names(which.max(table(x)))) %>%
factor(levels = c("Yes","No"))
accuracy_results %>% mutate(mean_acc_sens = (accuracy + sensitivity)/2) %>% arrange(mean_acc_sens) # top 3 is Naive Bayes, Decision Tree and Random Forest
all_preds <- tibble(nb = preds_nb,
tree = preds_tree,
rf  = preds_rf,
glm = preds_glm,
svm = preds_svm)
preds_ens <- apply(all_preds,1,function(x) names(which.max(table(x)))) %>%
factor(levels = c("Yes","No"))
cm_ens <- confusionMatrix(preds_ens, heart_test$PD)
# save accuracy and sensitivity
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Ensemble",
accuracy = cm_ens$overall["Accuracy"],
sensitivity = cm_ens$byClass["Sensitivity"]))
preds_ens <- apply(all_preds,1,function(x) names(which.max(table(x)))) %>%
factor(levels = c("Yes","No"))
cm_ens <- confusionMatrix(preds_ens, heart_test$PD)
# save accuracy and sensitivity
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Ensemble",
accuracy = cm_ens$overall["Accuracy"],
sensitivity = cm_ens$byClass["Sensitivity"
]))
all_preds <- tibble(nb = preds_nb,
tree = preds_tree,
rf  = preds_rf)
all_preds
preds_ens <- apply(all_preds,1,function(x) names(which.max(table(x)))) %>%
factor(levels = c("Yes","No"))
cm_ens <- confusionMatrix(preds_ens, heart_test$PD)
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Ensemble",
accuracy = cm_ens$overall["Accuracy"],
sensitivity = cm_ens$byClass["Sensitivity"]))
k <- 10
mtry = 1:11
accuracy_mtry_final <- matrix(nrow = k, ncol = length(mtry))
set.seed(1, sample.kind = "Rounding")
model_nb_final <- train(PD~.,
method = 'nb',
data = train,
trControl = trainControl(method = 'cv', number = 10))
#create predictions
preds_nb_final <- predict(model_nb_final, validation)
k <- 10
cp = seq(0,0.5,0.01)
accuracy_cp_final <- matrix(nrow = k, ncol = length(cp))
#create folds
set.seed(2005, sample.kind = "Rounding")
ind <- createFolds(1:nrow(train), k = k)
for (i in 1:k) {
# create train and test sets for cv
cv_train <- train %>% slice(-ind[[i]])
cv_test <- train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from glm with cutoff p
accuracy_cp_final[i,] <- sapply(cp, function(cp){
# create the decision tree
cv_mod_tree <- rpart(PD~., cp = cp, data =  cv_train, method = "class")
# obtain the predictions (these are probabilities)
cv_preds_tree <- predict(cv_mod_tree, cv_test, type = "class")
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_tree, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
#extract optimal cp
opt_cp_final <- median(cp[which(min_rank(desc(colMeans(accuracy_cp_final)))==1)])
#plot cp vs accuracy
tibble(cp = cp, mean_accuracy = colMeans(accuracy_cp_final)) %>%
ggplot(aes(cp, mean_accuracy)) +
geom_smooth()+
geom_point() +
geom_point(aes(opt_cp, max(mean_accuracy)), shape = 5, size = 5) +
xlab("Cutoff (cp)") +
ylab("Mean of Accuracy and Sensitivity") +
ggtitle("Mean of Accuracy and Sensitivity for Various complex. params")
#create model using optimal tree
model_tree_final <- rpart(PD~., cp = opt_cp, data =  train, method = "class")
#generate predictions
preds_tree_final <- predict(model_tree_final, validation, type = "class")
k <- 10
mtry = 1:11
accuracy_mtry_final <- matrix(nrow = k, ncol = length(mtry))
#create folds
set.seed(1, sample.kind = "Rounding")
ind <- createFolds(1:nrow(train), k = k)
#perform cv
for (i in 1:k) {
# create train and test sets for cv
cv_train <- train %>% slice(-ind[[i]])
cv_test <- train %>% slice(ind[[i]])
# fill matrix with results (mean of accuracy and sensitivity) from decision tree
accuracy_mtry_final[i,] <- sapply(mtry, function(m){
# create the decision tree
set.seed(1, sample.kind = "Rounding")
cv_mod_rf <- randomForest(PD ~.,
data = cv_train,
mtry = m)
# obtain the predictions (these are probabilities)
cv_preds_rf <- predict(cv_mod_rf, cv_test)
# if prediction>p, class as Positive.
cv_cm <- confusionMatrix(cv_preds_rf, cv_test$PD)
# return the mean of the accuracy and sensitivity
return(mean(c(cv_cm$overall["Accuracy"], cv_cm$byClass["Sensitivity"])))
})
# keep track of how many seeds have been run
cat("fold",i,"out of 10 complete\n")
}
#extract optimal mtry
opt_mtry_final <- median(mtry[which(min_rank(desc(colMeans(accuracy_mtry_final)))==1)])
model_rf_final <-randomForest(PD ~.,
data = train,
mtry = opt_mtry_final)
preds_rf_final <- predict(model_rf_final, validation)
all_preds_final <- tibble(nb = preds_nb_final,
tree = preds_tree_final,
rf  = preds_rf_final)
# the predictions of the ensemble are obtained by majority votes
preds_ens_final <- apply(all_preds_final,1,function(x) names(which.max(table(x)))) %>%
factor(levels = c("Yes","No"))
cm_ens_final <-confusionMatrix(preds_ens_final, validation$PD)
accuracy_results <- rbind(accuracy_results,
data.frame(Method = "Final Model: Ensemble (using validation set)",
accuracy = cm_ens_final$overall["Accuracy"],
sensitivity = cm_ens_final$byClass["Sensitivity"]))
confusionMatrix(preds_ens_final, validation$PD)$overall["Accuracy"]
confusionMatrix(preds_ens_final, validation$PD)$byClass["Sensitivity"]
confusionMatrix(preds_rf_final, validation$PD)
save(heart, file = "rda_files/structure.rda")
save(opt_p, opt_p_two, opt_cp, opt_mtry, opt_cost, file = "rda_files/opt_values.rda")
save(model_glm, model_glm_two, model_nb, model_tree, model_rf, model_svm, model_nb_final, model_tree_final, model_rf_final, file = "rda_files/models.rda" )
save(cm_glm, cm_glm_two, cm_nb, cm_tree, cm_rf, cm_svm, cm_ens, cm_ens_final, file = "rda_files/matrices.rda")
save(results, file = "rda_files/results.rda")
View(accuracy_results)
results
results <- tibble(Method = c("Logistic Regression", "Naive Bayes", "Decision Tree","Random Forest","SVM","Ensemble"),
Accuracy = c(confusionMatrix(preds_glm_two, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_nb, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_tree, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_rf, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_svm, heart_test$PD)$overall["Accuracy"],
confusionMatrix(preds_ens, heart_test$PD)$overall["Accuracy"]),
Sensitivity = c(confusionMatrix(preds_glm_two, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_nb, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_tree, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_rf, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_svm, heart_test$PD)$byClass["Sensitivity"],
confusionMatrix(preds_ens, heart_test$PD)$byClass["Sensitivity"])) %>%
mutate(Mean = rowMeans(dplyr::select(., Accuracy, Sensitivity)))
results
